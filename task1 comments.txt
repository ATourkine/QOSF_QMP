The optimization does not seem to converge to zero as the number of layers increases   .
I tried several optimization routines and I found that the noisy nature of the calculation is incompatible with any method that uses a gradient calculation.
Annealing optimization is taking too much time. Considering the high dimension of the problem I have not tried a brute force grid search.
The method that seemed to be the most stable turned out to be the SLSQP optimization routine in scipy.
As dimension grows with the number of layers growing the minimization becomes more unstable and we observe some fluctuations and worsening of the fit quality.
Using the optimal theta calculated at the previous step seems to improve the quality of the minimizer

For a moment I had a doubt about using the same vector of Theta for RX and RZ rotations. Not surprisingly that decreased the quality of the minimization.
Switching from RX to RY does not change the situation and the convergence and fit quality are comparable.
Switching from RX gates in odd layers to RZ precludes any optimization. No fit is possible as the rotations happen only in one dimension and so the optimization is greatly constrained.



